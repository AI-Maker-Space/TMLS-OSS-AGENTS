{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks!"
      ],
      "metadata": {
        "id": "gJXW_DgiSebM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ],
      "metadata": {
        "id": "e7pQDUhUnIo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ],
      "metadata": {
        "id": "3_fLDElOVoop"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaVwN269EttM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb533b8f-9bcb-4dda-dd94-157666e04ae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.6/91.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.9/357.9 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain_openai langchain_huggingface langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU pymupdf qdrant-client langchain_qdrant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu_G1BOMg8_e",
        "outputId": "b062361a-b509-48f8-b86b-dd99f7205973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.3/309.3 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-api-core 2.16.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-aiplatform 1.57.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-bigtable 2.24.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-iam 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-language 2.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.2 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.2 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ],
      "metadata": {
        "id": "wujPjGJuoPwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "f1be8042-0195-48c0-c8d7-16b5151e7155"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE3 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "e96441a8-2dd5-4ebb-f92e-2175902adf53"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open Source RAG Tool\n",
        "\n",
        "We'll leverage the previous tools we created to provide an Open Source RAG chain to be used as a tool."
      ],
      "metadata": {
        "id": "xNIsxCtygP2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "documents = PyMuPDFLoader(\"https://www.courthousenews.com/wp-content/uploads/2024/02/musk-v-altman-openai-complaint-sf.pdf\").load()"
      ],
      "metadata": {
        "id": "OcxvaSBHgYUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "\n",
        "eval_documents = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "THOL8jA5hFiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_EMBED_URL = \"https://eb08ghvx9kufhyjj.us-east-1.aws.endpoints.huggingface.cloud\""
      ],
      "metadata": {
        "id": "AlTyoDKHhGrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Hugging Face Token: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpSN9lV5hlJm",
        "outputId": "8f0b706f-106e-46a3-cedf-f1efd9ad95d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hugging Face Token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
        "\n",
        "embedding_model = HuggingFaceEndpointEmbeddings(\n",
        "    model=HF_EMBED_URL,\n",
        "    task=\"feature-extraction\",\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Cyo-sGDuhekC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "for i in range(0, len(documents), 32):\n",
        "  if i == 0:\n",
        "    vectorstore = Qdrant.from_documents(\n",
        "        eval_documents[i:i+32],\n",
        "        embedding_model,\n",
        "        location=\":memory:\",\n",
        "        collection_name=\"Elon's Complaint\")\n",
        "    continue\n",
        "  vectorstore.add_documents(eval_documents[i:i+32])"
      ],
      "metadata": {
        "id": "8q9BBMT0iEGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "pWoKw3zcicgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and question, you must answer the question based only on context.\n",
        "\n",
        "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ],
      "metadata": {
        "id": "uxp6ugj-ifsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_LLM_URL = \"https://meqlao8jlyg1d5w2.us-east-1.aws.endpoints.huggingface.cloud\" + \"/v1/\""
      ],
      "metadata": {
        "id": "-0enaqIksCyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "hf_llm = ChatOpenAI(\n",
        "    model=\"tgi\",\n",
        "    openai_api_base=HF_LLM_URL,\n",
        "    openai_api_key=os.environ[\"HF_TOKEN\"]\n",
        ")"
      ],
      "metadata": {
        "id": "fkJfnZRdrnIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | rag_prompt | hf_llm | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "wwa70aF7iixa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, List, Tuple, Union\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def retrieve_information(\n",
        "    query: Annotated[str, \"query to ask the retrieve information tool\"]\n",
        "    ):\n",
        "  \"\"\"Used to answer questions about the Elon Musk complaint against OpenAI.\"\"\"\n",
        "  return rag_chain.invoke({\"question\" : query})"
      ],
      "metadata": {
        "id": "dTCdTbn7sf30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ],
      "metadata": {
        "id": "sBRyQmEAVzua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun(),\n",
        "    retrieve_information,\n",
        "]"
      ],
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/fab950acfbf5fea46c9313dca34ee2ae01f1728b/libs/langgraph/langgraph/prebuilt/tool_executor.py#L50) to do so."
      ],
      "metadata": {
        "id": "1FdOjEslXdRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ],
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ],
      "metadata": {
        "id": "VI-C669ZYVI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(\n",
        "    model_name=\"gpt-4o\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=1024\n",
        ")"
      ],
      "metadata": {
        "id": "ekMHjpNfKWep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ],
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ],
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ],
      "metadata": {
        "id": "_296Ub96Z_H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ],
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ],
      "metadata": {
        "id": "vWsMhfO9grLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  print(response)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ],
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ],
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ],
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ],
      "metadata": {
        "id": "b8CjRlbVmRpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ],
      "metadata": {
        "id": "uaXHpPeSnOWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ],
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ],
      "metadata": {
        "id": "BUsfGoSpoF9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ],
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  print(last_message)\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ],
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ],
      "metadata": {
        "id": "yKCjWJCkrJb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ],
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ],
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ],
      "metadata": {
        "id": "KYqDpErlsCsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Function to print messages"
      ],
      "metadata": {
        "id": "GSCds6zTL5VJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_messages(messages):\n",
        "  next_is_tool = False\n",
        "  initial_query = True\n",
        "  for message in messages[\"messages\"]:\n",
        "    if \"function_call\" in message.additional_kwargs:\n",
        "      print()\n",
        "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
        "      next_is_tool = True\n",
        "      continue\n",
        "    if next_is_tool:\n",
        "      print(f\"Tool Response: {message.content}\")\n",
        "      next_is_tool = False\n",
        "      continue\n",
        "    if initial_query:\n",
        "      print(f\"Initial Query: {message.content}\")\n",
        "      print()\n",
        "      initial_query = False\n",
        "      continue\n",
        "    print()\n",
        "    print(f\"Agent Response: {message.content}\")"
      ],
      "metadata": {
        "id": "xRPF0X5iL8Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ],
      "metadata": {
        "id": "VEYcTShCsPaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is the Elon Musk complaint against OpenAI about?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "4532210f-1f91-4829-d40e-1095fd42e490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='' additional_kwargs={'function_call': {'arguments': '{\"query\":\"Elon Musk complaint against OpenAI\"}', 'name': 'retrieve_information'}} response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 193, 'total_tokens': 213}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None} id='run-da556236-0c22-4f06-87d8-8611a366530b-0' usage_metadata={'input_tokens': 193, 'output_tokens': 20, 'total_tokens': 213}\n",
            "content='' additional_kwargs={'function_call': {'arguments': '{\"query\":\"Elon Musk complaint against OpenAI\"}', 'name': 'retrieve_information'}} response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 193, 'total_tokens': 213}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None} id='run-da556236-0c22-4f06-87d8-8611a366530b-0' usage_metadata={'input_tokens': 193, 'output_tokens': 20, 'total_tokens': 213}\n",
            "content=\"Elon Musk's complaint against OpenAI centers on concerns that the organization is deviating from its original mission of developing artificial general intelligence (AGI) for the benefit of humanity. Musk argues that OpenAI should adhere to its founding principles and not prioritize the personal benefit of individuals or commercial interests, such as those of Microsoft. He is particularly concerned that OpenAI's collaboration with Microsoft has led to the development of closed-source technology, which he believes undermines the commitment to public benefit.\" response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 316, 'total_tokens': 414}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_dd932ca5d1', 'finish_reason': 'stop', 'logprobs': None} id='run-c78f1956-303f-482c-b171-4c965ac0777a-0' usage_metadata={'input_tokens': 316, 'output_tokens': 98, 'total_tokens': 414}\n",
            "content=\"Elon Musk's complaint against OpenAI centers on concerns that the organization is deviating from its original mission of developing artificial general intelligence (AGI) for the benefit of humanity. Musk argues that OpenAI should adhere to its founding principles and not prioritize the personal benefit of individuals or commercial interests, such as those of Microsoft. He is particularly concerned that OpenAI's collaboration with Microsoft has led to the development of closed-source technology, which he believes undermines the commitment to public benefit.\" response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 316, 'total_tokens': 414}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_dd932ca5d1', 'finish_reason': 'stop', 'logprobs': None} id='run-c78f1956-303f-482c-b171-4c965ac0777a-0' usage_metadata={'input_tokens': 316, 'output_tokens': 98, 'total_tokens': 414}\n",
            "Initial Query: What is the Elon Musk complaint against OpenAI about?\n",
            "\n",
            "\n",
            "Tool Call - Name: retrieve_information + Query: {\"query\":\"Elon Musk complaint against OpenAI\"}\n",
            "Tool Response: Elon Musk's complaint against OpenAI is related to concerns over the development and distribution of advanced artificial general intelligence (AGI) technology. He believes that OpenAI should adhere to their founding agreement and focus on developing AGI for the benefit of humanity, rather than for the personal benefit of the individual defendants and Microsoft. The complaint also alleges that OpenAI's collaboration with Microsoft has resulted in the development of closed-source technology, which prioritizes proprietary commercial interests over public benefit.\n",
            "\n",
            "Agent Response: Elon Musk's complaint against OpenAI centers on concerns that the organization is deviating from its original mission of developing artificial general intelligence (AGI) for the benefit of humanity. Musk argues that OpenAI should adhere to its founding principles and not prioritize the personal benefit of individuals or commercial interests, such as those of Microsoft. He is particularly concerned that OpenAI's collaboration with Microsoft has led to the development of closed-source technology, which he believes undermines the commitment to public benefit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ],
      "metadata": {
        "id": "DBHnUtLSscRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Did Elon Musk ever beef with Mark Zuckerberg? Was there ever a legal complaint filed by Elon Musk? Use all your tools to answer these questions.\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "eb2d15e5-fb12-4370-f429-52c55aaf1770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='' additional_kwargs={'function_call': {'arguments': '{\"query\":\"Elon Musk Mark Zuckerberg feud\"}', 'name': 'duckduckgo_search'}} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 211, 'total_tokens': 233}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None} id='run-114a71b7-beca-4496-916a-381367450882-0' usage_metadata={'input_tokens': 211, 'output_tokens': 22, 'total_tokens': 233}\n",
            "content='' additional_kwargs={'function_call': {'arguments': '{\"query\":\"Elon Musk Mark Zuckerberg feud\"}', 'name': 'duckduckgo_search'}} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 211, 'total_tokens': 233}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None} id='run-114a71b7-beca-4496-916a-381367450882-0' usage_metadata={'input_tokens': 211, 'output_tokens': 22, 'total_tokens': 233}\n",
            "content=\"Elon Musk and Mark Zuckerberg have had a public feud that has included passive-aggressive jabs and even the suggestion of a cage match. Their rivalry has been ongoing for several years, with various incidents escalating the tension between the two tech billionaires.\\n\\nNow, I'll check if there was ever a legal complaint filed by Elon Musk.\" additional_kwargs={'function_call': {'arguments': '{\"query\":\"Elon Musk legal complaint\"}', 'name': 'retrieve_information'}} response_metadata={'token_usage': {'completion_tokens': 85, 'prompt_tokens': 459, 'total_tokens': 544}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None} id='run-191f81d9-b126-45f7-848a-de9174c98bcf-0' usage_metadata={'input_tokens': 459, 'output_tokens': 85, 'total_tokens': 544}\n",
            "content=\"Elon Musk and Mark Zuckerberg have had a public feud that has included passive-aggressive jabs and even the suggestion of a cage match. Their rivalry has been ongoing for several years, with various incidents escalating the tension between the two tech billionaires.\\n\\nNow, I'll check if there was ever a legal complaint filed by Elon Musk.\" additional_kwargs={'function_call': {'arguments': '{\"query\":\"Elon Musk legal complaint\"}', 'name': 'retrieve_information'}} response_metadata={'token_usage': {'completion_tokens': 85, 'prompt_tokens': 459, 'total_tokens': 544}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None} id='run-191f81d9-b126-45f7-848a-de9174c98bcf-0' usage_metadata={'input_tokens': 459, 'output_tokens': 85, 'total_tokens': 544}\n",
            "content='' additional_kwargs={'function_call': {'arguments': '{\"query\":\"Elon Musk legal complaint history\"}', 'name': 'duckduckgo_search'}} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 588, 'total_tokens': 610}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None} id='run-5283666a-68bc-44b9-8a0c-b3dcaed32832-0' usage_metadata={'input_tokens': 588, 'output_tokens': 22, 'total_tokens': 610}\n",
            "content='' additional_kwargs={'function_call': {'arguments': '{\"query\":\"Elon Musk legal complaint history\"}', 'name': 'duckduckgo_search'}} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 588, 'total_tokens': 610}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None} id='run-5283666a-68bc-44b9-8a0c-b3dcaed32832-0' usage_metadata={'input_tokens': 588, 'output_tokens': 22, 'total_tokens': 610}\n",
            "content='Elon Musk has been involved in numerous legal issues, including lawsuits and investigations related to his companies and personal actions. However, there is no specific mention of a legal complaint filed by Elon Musk against Mark Zuckerberg or related to their feud.\\n\\nTo summarize:\\n1. **Feud with Mark Zuckerberg**: Elon Musk and Mark Zuckerberg have had a public feud, including passive-aggressive jabs and the suggestion of a cage match.\\n2. **Legal Complaints**: Elon Musk has been involved in various legal issues, but there is no specific record of a legal complaint filed by him against Mark Zuckerberg.' response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 814, 'total_tokens': 935}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None} id='run-c3b6feb2-e026-4b8c-99c9-54444d4de98a-0' usage_metadata={'input_tokens': 814, 'output_tokens': 121, 'total_tokens': 935}\n",
            "content='Elon Musk has been involved in numerous legal issues, including lawsuits and investigations related to his companies and personal actions. However, there is no specific mention of a legal complaint filed by Elon Musk against Mark Zuckerberg or related to their feud.\\n\\nTo summarize:\\n1. **Feud with Mark Zuckerberg**: Elon Musk and Mark Zuckerberg have had a public feud, including passive-aggressive jabs and the suggestion of a cage match.\\n2. **Legal Complaints**: Elon Musk has been involved in various legal issues, but there is no specific record of a legal complaint filed by him against Mark Zuckerberg.' response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 814, 'total_tokens': 935}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None} id='run-c3b6feb2-e026-4b8c-99c9-54444d4de98a-0' usage_metadata={'input_tokens': 814, 'output_tokens': 121, 'total_tokens': 935}\n",
            "Initial Query: Did Elon Musk ever beef with Mark Zuckerberg? Was there ever a legal complaint filed by Elon Musk? Use all your tools to answer these questions.\n",
            "\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Elon Musk Mark Zuckerberg feud\"}\n",
            "Tool Response: Elon Musk and Mark Zuckerberg have threatened to take their feud to the next level and face each other in a cage match. Susan Walsh/AP; Erin Scott/Reuters. There's no love lost between Elon Musk ... Elon Musk and Mark Zuckerberg have been making passive-aggressive jabs at each other for several years, but the feud between the two tech titans has grown increasingly childish. Musk boasts an ... How did the public feud between Mark Zuckerberg and Elon Musk escalate from a SpaceX failure to a shirtless photo post? See the key moments and tweets of the snarky rivalry between the two influential tech billionaires. In the battle of Musk vs. Zuck, Zuckerberg is finally winning — for now. Charissa Cheong. Dec 22, 2023, 5:19 AM PST. The reputations of both Elon Musk and Mark Zuckerberg took unexpected turns ... reader comments 330. The cage match between Mark Zuckerberg and Elon Musk is over before it even began. In a series of posts on Threads, Meta CEO Zuckerberg said he was done talking about the ...\n",
            "\n",
            "Tool Call - Name: retrieve_information + Query: {\"query\":\"Elon Musk legal complaint\"}\n",
            "Tool Response: I don't know. The provided context only contains a brief excerpt from a document and does not give enough information to identify the specific legal complaint Elon Musk is involved in.\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Elon Musk legal complaint history\"}\n",
            "Tool Response: Elon Musk has a lot of legal problems. BI scoured the dockets to find the biggest court cases and investigations posing a threat to the billionaire. If Musk throws his support behind Donald Trump ... Elon Musk has a lot of legal problems. BI scoured the dockets to find the biggest court cases and investigations posing a threat to the billionaire. If Musk throws his support behind Donald Trump ... July-September 2022. Twitter sued Elon Musk in Delaware's Chancery Court to enforce the completion of his $44 billion acquisition of the company. The legal action followed Musk's attempt to ... Elon Musk won dismissal of a lawsuit claiming he refused to pay at least $500 million of severance to thousands of Twitter employees he fired in mass layoffs after buying the social media company ... Elon Musk and his companies are embroiled in dozens of lawsuits, ranging from fatal crashes blamed on Tesla Inc.'s Autopilot system to civil rights complaints over the treatment of employees.\n",
            "\n",
            "Agent Response: Elon Musk has been involved in numerous legal issues, including lawsuits and investigations related to his companies and personal actions. However, there is no specific mention of a legal complaint filed by Elon Musk against Mark Zuckerberg or related to their feud.\n",
            "\n",
            "To summarize:\n",
            "1. **Feud with Mark Zuckerberg**: Elon Musk and Mark Zuckerberg have had a public feud, including passive-aggressive jabs and the suggestion of a cage match.\n",
            "2. **Legal Complaints**: Elon Musk has been involved in various legal issues, but there is no specific record of a legal complaint filed by him against Mark Zuckerberg.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: LangSmith Evaluator"
      ],
      "metadata": {
        "id": "v7c8-Uyarh1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing for LangSmith"
      ],
      "metadata": {
        "id": "pV3XeFOT1Sar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ],
      "metadata": {
        "id": "wruQCuzewUuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ],
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.invoke({\"question\" : \"What is QLoRA?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "19d7df29-6ec2-4a80-e589-6cd3a80bc81c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='QLoRA, or Quantized Low-Rank Adaptation, is a technique used in the field of machine learning, particularly in the context of fine-tuning large language models. The primary goal of QLoRA is to reduce the computational and memory requirements associated with fine-tuning large models, making it more efficient and accessible.\\n\\nHere are some key points about QLoRA:\\n\\n1. **Quantization**: QLoRA involves quantizing the weights of the neural network. Quantization is the process of reducing the precision of the weights, typically from 32-bit floating-point numbers to lower precision formats like 8-bit integers. This reduces the memory footprint and computational load.\\n\\n2. **Low-Rank Adaptation**: In addition to quantization, QLoRA employs low-rank adaptation techniques. This involves approximating the weight matrices of the neural network with low-rank matrices. By doing so, the number of parameters that need to be fine-tuned is significantly reduced.\\n\\n3. **Efficiency**: By combining quantization and low-rank adaptation, QLoRA achieves a more efficient fine-tuning process. This allows for faster training times and lower resource consumption, which is particularly beneficial when working with very large models.\\n\\n4. **Performance**: Despite the reduction in precision and the use of low-rank approximations, QLoRA aims to maintain the performance of the fine-tuned model. The goal is to achieve similar or even better results compared to traditional fine-tuning methods, but with much lower computational costs.\\n\\nOverall, QLoRA is a valuable technique for making the fine-tuning of large language models more practical and cost-effective, enabling broader access to advanced machine learning capabilities.' response_metadata={'token_usage': {'completion_tokens': 344, 'prompt_tokens': 188, 'total_tokens': 532}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None} id='run-e0ef9d3e-2eab-4365-b87c-e6295b9a805e-0' usage_metadata={'input_tokens': 188, 'output_tokens': 344, 'total_tokens': 532}\n",
            "content='QLoRA, or Quantized Low-Rank Adaptation, is a technique used in the field of machine learning, particularly in the context of fine-tuning large language models. The primary goal of QLoRA is to reduce the computational and memory requirements associated with fine-tuning large models, making it more efficient and accessible.\\n\\nHere are some key points about QLoRA:\\n\\n1. **Quantization**: QLoRA involves quantizing the weights of the neural network. Quantization is the process of reducing the precision of the weights, typically from 32-bit floating-point numbers to lower precision formats like 8-bit integers. This reduces the memory footprint and computational load.\\n\\n2. **Low-Rank Adaptation**: In addition to quantization, QLoRA employs low-rank adaptation techniques. This involves approximating the weight matrices of the neural network with low-rank matrices. By doing so, the number of parameters that need to be fine-tuned is significantly reduced.\\n\\n3. **Efficiency**: By combining quantization and low-rank adaptation, QLoRA achieves a more efficient fine-tuning process. This allows for faster training times and lower resource consumption, which is particularly beneficial when working with very large models.\\n\\n4. **Performance**: Despite the reduction in precision and the use of low-rank approximations, QLoRA aims to maintain the performance of the fine-tuned model. The goal is to achieve similar or even better results compared to traditional fine-tuning methods, but with much lower computational costs.\\n\\nOverall, QLoRA is a valuable technique for making the fine-tuning of large language models more practical and cost-effective, enabling broader access to advanced machine learning capabilities.' response_metadata={'token_usage': {'completion_tokens': 344, 'prompt_tokens': 188, 'total_tokens': 532}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None} id='run-e0ef9d3e-2eab-4365-b87c-e6295b9a805e-0' usage_metadata={'input_tokens': 188, 'output_tokens': 344, 'total_tokens': 532}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'QLoRA, or Quantized Low-Rank Adaptation, is a technique used in the field of machine learning, particularly in the context of fine-tuning large language models. The primary goal of QLoRA is to reduce the computational and memory requirements associated with fine-tuning large models, making it more efficient and accessible.\\n\\nHere are some key points about QLoRA:\\n\\n1. **Quantization**: QLoRA involves quantizing the weights of the neural network. Quantization is the process of reducing the precision of the weights, typically from 32-bit floating-point numbers to lower precision formats like 8-bit integers. This reduces the memory footprint and computational load.\\n\\n2. **Low-Rank Adaptation**: In addition to quantization, QLoRA employs low-rank adaptation techniques. This involves approximating the weight matrices of the neural network with low-rank matrices. By doing so, the number of parameters that need to be fine-tuned is significantly reduced.\\n\\n3. **Efficiency**: By combining quantization and low-rank adaptation, QLoRA achieves a more efficient fine-tuning process. This allows for faster training times and lower resource consumption, which is particularly beneficial when working with very large models.\\n\\n4. **Performance**: Despite the reduction in precision and the use of low-rank approximations, QLoRA aims to maintain the performance of the fine-tuned model. The goal is to achieve similar or even better results compared to traditional fine-tuning methods, but with much lower computational costs.\\n\\nOverall, QLoRA is a valuable technique for making the fine-tuning of large language models more practical and cost-effective, enabling broader access to advanced machine learning capabilities.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below."
      ],
      "metadata": {
        "id": "f9UkCIqkpyZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####🏗️ Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ],
      "metadata": {
        "id": "VfMXF2KAsQxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]"
      ],
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ],
      "metadata": {
        "id": "z7QVFuAmsh7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ],
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ],
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ],
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ],
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ],
      "metadata": {
        "id": "r1RJr349zhv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "4329a1e4-5074-467f-e46c-e8172d1c693a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 34472ee0' at:\n",
            "https://smith.langchain.com/o/340cd80b-3296-5752-9a9e-58582118073a/datasets/3f5732ac-645c-4180-b948-1429ee5f2878/compare?selectedSessions=b28f8953-8bd1-4c02-9145-ebfe310023c1\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - cd4b1eb9 at:\n",
            "https://smith.langchain.com/o/340cd80b-3296-5752-9a9e-58582118073a/datasets/3f5732ac-645c-4180-b948-1429ee5f2878\n",
            "[>                                                 ] 0/6content='' additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA data type\"}', 'name': 'arxiv'}} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 194, 'total_tokens': 212}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None} id='run-1d3c08b0-502d-43d5-b77d-03151e53f656-0' usage_metadata={'input_tokens': 194, 'output_tokens': 18, 'total_tokens': 212}\n",
            "content='' additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA data type\"}', 'name': 'arxiv'}} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 194, 'total_tokens': 212}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None} id='run-1d3c08b0-502d-43d5-b77d-03151e53f656-0' usage_metadata={'input_tokens': 194, 'output_tokens': 18, 'total_tokens': 212}\n",
            "content='' additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA\"}', 'name': 'arxiv'}} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 190, 'total_tokens': 206}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None} id='run-ea0145b3-f23d-47b6-8e2e-9364d2f82815-0' usage_metadata={'input_tokens': 190, 'output_tokens': 16, 'total_tokens': 206}\n",
            "content='' additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA\"}', 'name': 'arxiv'}} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 190, 'total_tokens': 206}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None} id='run-ea0145b3-f23d-47b6-8e2e-9364d2f82815-0' usage_metadata={'input_tokens': 190, 'output_tokens': 16, 'total_tokens': 206}\n",
            "content='QLoRA (Quantized Low-Rank Adaptation) typically uses the AdamW optimizer. AdamW is a variant of the Adam optimizer that includes weight decay for regularization. This optimizer is well-suited for training large language models and is commonly used in fine-tuning tasks, including those involving quantized models like QLoRA.' response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 191, 'total_tokens': 260}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_298125635f', 'finish_reason': 'stop', 'logprobs': None} id='run-8ba03438-99f6-478c-af86-43f4e457d106-0' usage_metadata={'input_tokens': 191, 'output_tokens': 69, 'total_tokens': 260}\n",
            "content='QLoRA (Quantized Low-Rank Adaptation) typically uses the AdamW optimizer. AdamW is a variant of the Adam optimizer that includes weight decay for regularization. This optimizer is well-suited for training large language models and is commonly used in fine-tuning tasks, including those involving quantized models like QLoRA.' response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 191, 'total_tokens': 260}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_298125635f', 'finish_reason': 'stop', 'logprobs': None} id='run-8ba03438-99f6-478c-af86-43f4e457d106-0' usage_metadata={'input_tokens': 191, 'output_tokens': 69, 'total_tokens': 260}\n",
            "content='The QLoRA paper titled \"QLoRA: Efficient Finetuning of Quantized LLMs\" was authored by:\\n\\n- Tim Dettmers\\n- Artidoro Pagnoni\\n- Ari Holtzman\\n- Luke Zettlemoyer\\n\\nThe paper was published on May 23, 2023.' response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 1169, 'total_tokens': 1236}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_dd932ca5d1', 'finish_reason': 'stop', 'logprobs': None} id='run-942900e4-46a0-470a-a0b9-beb15b3ae33a-0' usage_metadata={'input_tokens': 1169, 'output_tokens': 67, 'total_tokens': 1236}\n",
            "content='The QLoRA paper titled \"QLoRA: Efficient Finetuning of Quantized LLMs\" was authored by:\\n\\n- Tim Dettmers\\n- Artidoro Pagnoni\\n- Ari Holtzman\\n- Luke Zettlemoyer\\n\\nThe paper was published on May 23, 2023.' response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 1169, 'total_tokens': 1236}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_dd932ca5d1', 'finish_reason': 'stop', 'logprobs': None} id='run-942900e4-46a0-470a-a0b9-beb15b3ae33a-0' usage_metadata={'input_tokens': 1169, 'output_tokens': 67, 'total_tokens': 1236}\n",
            "content='In the QLoRA paper, the authors introduced a new data type called **4-bit NormalFloat (NF4)**. This data type is designed to be information-theoretically optimal for normally distributed weights, which helps in reducing memory usage while preserving performance during the fine-tuning of large language models.' response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 1107, 'total_tokens': 1169}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None} id='run-0d054f53-909b-428e-ab55-1158ed2bf6fe-0' usage_metadata={'input_tokens': 1107, 'output_tokens': 62, 'total_tokens': 1169}\n",
            "content='In the QLoRA paper, the authors introduced a new data type called **4-bit NormalFloat (NF4)**. This data type is designed to be information-theoretically optimal for normally distributed weights, which helps in reducing memory usage while preserving performance during the fine-tuning of large language models.' response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 1107, 'total_tokens': 1169}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None} id='run-0d054f53-909b-428e-ab55-1158ed2bf6fe-0' usage_metadata={'input_tokens': 1107, 'output_tokens': 62, 'total_tokens': 1169}\n",
            "content=\"The popularity of deep learning frameworks can vary depending on the criteria used (e.g., industry adoption, academic research, community support, etc.). However, as of the most recent data, the following frameworks are among the most popular:\\n\\n1. **TensorFlow**: Developed by Google, TensorFlow is widely used in both industry and academia. It has a comprehensive ecosystem and supports a wide range of applications.\\n\\n2. **PyTorch**: Developed by Facebook's AI Research lab, PyTorch has gained significant popularity, especially in the research community, due to its dynamic computation graph and ease of use.\\n\\n3. **Keras**: Initially an independent project, Keras is now part of the TensorFlow ecosystem. It is known for its simplicity and ease of use, making it a popular choice for beginners.\\n\\n4. **Caffe**: Developed by the Berkeley Vision and Learning Center (BVLC), Caffe is known for its speed and efficiency, particularly in image processing tasks.\\n\\n5. **MXNet**: An open-source deep learning framework developed by Apache, MXNet is known for its scalability and efficiency, particularly in distributed computing environments.\\n\\n6. **JAX**: Developed by Google, JAX is gaining traction for its ability to transform numerical functions into high-performance, GPU/TPU-executable code.\\n\\nTo get the most current and specific information, including trends and statistics, I can perform a search. Would you like me to do that?\" response_metadata={'token_usage': {'completion_tokens': 294, 'prompt_tokens': 191, 'total_tokens': 485}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_dd932ca5d1', 'finish_reason': 'stop', 'logprobs': None} id='run-dec1ab3a-51fa-406d-bf63-e726fb6e9446-0' usage_metadata={'input_tokens': 191, 'output_tokens': 294, 'total_tokens': 485}\n",
            "content=\"The popularity of deep learning frameworks can vary depending on the criteria used (e.g., industry adoption, academic research, community support, etc.). However, as of the most recent data, the following frameworks are among the most popular:\\n\\n1. **TensorFlow**: Developed by Google, TensorFlow is widely used in both industry and academia. It has a comprehensive ecosystem and supports a wide range of applications.\\n\\n2. **PyTorch**: Developed by Facebook's AI Research lab, PyTorch has gained significant popularity, especially in the research community, due to its dynamic computation graph and ease of use.\\n\\n3. **Keras**: Initially an independent project, Keras is now part of the TensorFlow ecosystem. It is known for its simplicity and ease of use, making it a popular choice for beginners.\\n\\n4. **Caffe**: Developed by the Berkeley Vision and Learning Center (BVLC), Caffe is known for its speed and efficiency, particularly in image processing tasks.\\n\\n5. **MXNet**: An open-source deep learning framework developed by Apache, MXNet is known for its scalability and efficiency, particularly in distributed computing environments.\\n\\n6. **JAX**: Developed by Google, JAX is gaining traction for its ability to transform numerical functions into high-performance, GPU/TPU-executable code.\\n\\nTo get the most current and specific information, including trends and statistics, I can perform a search. Would you like me to do that?\" response_metadata={'token_usage': {'completion_tokens': 294, 'prompt_tokens': 191, 'total_tokens': 485}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_dd932ca5d1', 'finish_reason': 'stop', 'logprobs': None} id='run-dec1ab3a-51fa-406d-bf63-e726fb6e9446-0' usage_metadata={'input_tokens': 191, 'output_tokens': 294, 'total_tokens': 485}\n",
            "content=\"A Retrieval Augmented Generation (RAG) system is a type of artificial intelligence that combines retrieval-based and generation-based approaches to improve the quality and accuracy of generated responses. Here's a breakdown of its components and how it works:\\n\\n1. **Retrieval Component**: This part of the system searches a large corpus of documents or a database to find relevant information based on the input query. It retrieves the most pertinent pieces of information that can help answer the query.\\n\\n2. **Generation Component**: This part uses a generative model, such as a transformer-based language model (e.g., GPT-3), to generate a coherent and contextually appropriate response. The generative model can create new text based on the input and the retrieved information.\\n\\n3. **Integration**: The retrieved information is fed into the generative model to guide the generation process. This helps the model produce more accurate and informative responses by grounding its output in real, relevant data.\\n\\n### Benefits of RAG Systems\\n- **Improved Accuracy**: By leveraging external knowledge, RAG systems can provide more accurate and factually correct responses.\\n- **Contextual Relevance**: The retrieval component ensures that the generated responses are contextually relevant to the query.\\n- **Scalability**: These systems can handle a wide range of topics by accessing large and diverse datasets.\\n\\n### Applications\\n- **Question Answering**: Providing precise answers to user queries by retrieving relevant documents and generating a concise response.\\n- **Customer Support**: Assisting customer service agents by retrieving relevant information from knowledge bases and generating helpful responses.\\n- **Content Creation**: Aiding in the creation of articles, reports, or other content by retrieving relevant information and generating coherent text.\\n\\nOverall, RAG systems represent a powerful approach to combining the strengths of retrieval-based and generation-based methods to enhance the capabilities of AI in various applications.\" response_metadata={'token_usage': {'completion_tokens': 378, 'prompt_tokens': 191, 'total_tokens': 569}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_298125635f', 'finish_reason': 'stop', 'logprobs': None} id='run-3ca9b89d-7513-4f8f-bbdf-f2918b875eef-0' usage_metadata={'input_tokens': 191, 'output_tokens': 378, 'total_tokens': 569}\n",
            "content=\"A Retrieval Augmented Generation (RAG) system is a type of artificial intelligence that combines retrieval-based and generation-based approaches to improve the quality and accuracy of generated responses. Here's a breakdown of its components and how it works:\\n\\n1. **Retrieval Component**: This part of the system searches a large corpus of documents or a database to find relevant information based on the input query. It retrieves the most pertinent pieces of information that can help answer the query.\\n\\n2. **Generation Component**: This part uses a generative model, such as a transformer-based language model (e.g., GPT-3), to generate a coherent and contextually appropriate response. The generative model can create new text based on the input and the retrieved information.\\n\\n3. **Integration**: The retrieved information is fed into the generative model to guide the generation process. This helps the model produce more accurate and informative responses by grounding its output in real, relevant data.\\n\\n### Benefits of RAG Systems\\n- **Improved Accuracy**: By leveraging external knowledge, RAG systems can provide more accurate and factually correct responses.\\n- **Contextual Relevance**: The retrieval component ensures that the generated responses are contextually relevant to the query.\\n- **Scalability**: These systems can handle a wide range of topics by accessing large and diverse datasets.\\n\\n### Applications\\n- **Question Answering**: Providing precise answers to user queries by retrieving relevant documents and generating a concise response.\\n- **Customer Support**: Assisting customer service agents by retrieving relevant information from knowledge bases and generating helpful responses.\\n- **Content Creation**: Aiding in the creation of articles, reports, or other content by retrieving relevant information and generating coherent text.\\n\\nOverall, RAG systems represent a powerful approach to combining the strengths of retrieval-based and generation-based methods to enhance the capabilities of AI in various applications.\" response_metadata={'token_usage': {'completion_tokens': 378, 'prompt_tokens': 191, 'total_tokens': 569}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_298125635f', 'finish_reason': 'stop', 'logprobs': None} id='run-3ca9b89d-7513-4f8f-bbdf-f2918b875eef-0' usage_metadata={'input_tokens': 191, 'output_tokens': 378, 'total_tokens': 569}\n",
            "[----------------------------------------->        ] 5/6content='LoRA, or Low-Rank Adaptation, is a technique used to fine-tune large language models (LLMs) efficiently. Here are some significant improvements that LoRA brings to the table:\\n\\n1. **Parameter Efficiency**: LoRA significantly reduces the number of trainable parameters required for fine-tuning. Instead of updating all the parameters of the model, LoRA introduces a small number of additional parameters that capture the necessary adaptations. This makes the fine-tuning process much more parameter-efficient.\\n\\n2. **Memory Efficiency**: By reducing the number of parameters that need to be updated, LoRA also reduces the memory footprint during training. This allows for fine-tuning on hardware with limited memory resources, such as GPUs with smaller VRAM.\\n\\n3. **Training Speed**: With fewer parameters to update, the training process becomes faster. This is particularly beneficial when working with very large models, where full fine-tuning can be computationally expensive and time-consuming.\\n\\n4. **Modularity**: LoRA allows for modular updates to the model. This means that different tasks can have their own low-rank adaptations, which can be combined or swapped out as needed. This modularity is useful for multi-task learning and transfer learning scenarios.\\n\\n5. **Preservation of Pre-trained Knowledge**: Since LoRA updates only a small subset of parameters, the original pre-trained knowledge of the model is largely preserved. This helps in maintaining the generalization capabilities of the model while adapting it to specific tasks.\\n\\n6. **Scalability**: LoRA can be applied to very large models, making it scalable to state-of-the-art LLMs with billions of parameters. This scalability is crucial for leveraging the power of large models without incurring prohibitive computational costs.\\n\\n7. **Flexibility**: LoRA can be used in conjunction with other fine-tuning techniques, providing flexibility in how models are adapted to new tasks. It can be integrated into existing training pipelines with minimal changes.\\n\\nOverall, LoRA provides a more efficient and scalable way to fine-tune large language models, making it a valuable tool for researchers and practitioners working with LLMs.' response_metadata={'token_usage': {'completion_tokens': 433, 'prompt_tokens': 192, 'total_tokens': 625}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None} id='run-c6768c84-b942-4d6c-bc92-4f0e2c008e6d-0' usage_metadata={'input_tokens': 192, 'output_tokens': 433, 'total_tokens': 625}\n",
            "content='LoRA, or Low-Rank Adaptation, is a technique used to fine-tune large language models (LLMs) efficiently. Here are some significant improvements that LoRA brings to the table:\\n\\n1. **Parameter Efficiency**: LoRA significantly reduces the number of trainable parameters required for fine-tuning. Instead of updating all the parameters of the model, LoRA introduces a small number of additional parameters that capture the necessary adaptations. This makes the fine-tuning process much more parameter-efficient.\\n\\n2. **Memory Efficiency**: By reducing the number of parameters that need to be updated, LoRA also reduces the memory footprint during training. This allows for fine-tuning on hardware with limited memory resources, such as GPUs with smaller VRAM.\\n\\n3. **Training Speed**: With fewer parameters to update, the training process becomes faster. This is particularly beneficial when working with very large models, where full fine-tuning can be computationally expensive and time-consuming.\\n\\n4. **Modularity**: LoRA allows for modular updates to the model. This means that different tasks can have their own low-rank adaptations, which can be combined or swapped out as needed. This modularity is useful for multi-task learning and transfer learning scenarios.\\n\\n5. **Preservation of Pre-trained Knowledge**: Since LoRA updates only a small subset of parameters, the original pre-trained knowledge of the model is largely preserved. This helps in maintaining the generalization capabilities of the model while adapting it to specific tasks.\\n\\n6. **Scalability**: LoRA can be applied to very large models, making it scalable to state-of-the-art LLMs with billions of parameters. This scalability is crucial for leveraging the power of large models without incurring prohibitive computational costs.\\n\\n7. **Flexibility**: LoRA can be used in conjunction with other fine-tuning techniques, providing flexibility in how models are adapted to new tasks. It can be integrated into existing training pipelines with minimal changes.\\n\\nOverall, LoRA provides a more efficient and scalable way to fine-tune large language models, making it a valuable tool for researchers and practitioners working with LLMs.' response_metadata={'token_usage': {'completion_tokens': 433, 'prompt_tokens': 192, 'total_tokens': 625}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None} id='run-c6768c84-b942-4d6c-bc92-4f0e2c008e6d-0' usage_metadata={'input_tokens': 192, 'output_tokens': 433, 'total_tokens': 625}\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.helpfulness  feedback.COT Contextual Accuracy feedback.must_mention error  execution_time                                run_id\n",
            "count                   6.00                              5.00                     6     0            6.00                                     6\n",
            "unique                   NaN                               NaN                     2     0             NaN                                     6\n",
            "top                      NaN                               NaN                  True   NaN             NaN  c138db02-28bf-40fd-bd00-da788456c13c\n",
            "freq                     NaN                               NaN                     5   NaN             NaN                                     1\n",
            "mean                    1.00                              1.00                   NaN   NaN            5.29                                   NaN\n",
            "std                     0.00                              0.00                   NaN   NaN            3.02                                   NaN\n",
            "min                     1.00                              1.00                   NaN   NaN            1.96                                   NaN\n",
            "25%                     1.00                              1.00                   NaN   NaN            3.12                                   NaN\n",
            "50%                     1.00                              1.00                   NaN   NaN            4.53                                   NaN\n",
            "75%                     1.00                              1.00                   NaN   NaN            7.76                                   NaN\n",
            "max                     1.00                              1.00                   NaN   NaN            9.24                                   NaN\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - 34472ee0',\n",
              " 'results': {'6ea33375-e863-473f-bc37-ff1d476f8967': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\n1. The submission provides a direct answer to the question, stating that the AdamW optimizer is used in QLoRA. This is helpful as it directly addresses the query.\\n\\n2. The submission goes beyond just naming the optimizer, it also provides additional information about the AdamW optimizer, explaining that it is a variant of the Adam optimizer and includes weight decay for regularization. This is insightful as it provides more context about the optimizer.\\n\\n3. The submission also explains why the AdamW optimizer is used in QLoRA, stating that it is well-suited for training large language models and is commonly used in fine-tuning tasks, including those involving quantized models like QLoRA. This is appropriate as it provides a rationale for the use of the AdamW optimizer in QLoRA.\\n\\nBased on these points, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ce9d4cf5-25e5-42bf-bd93-2c1f36aa884a'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=None, value=None, comment=\"The context does not provide any information about the optimizer used in QLoRA. However, the student's answer states that QLoRA uses the AdamW optimizer. Without any conflicting information in the context, we can't definitively say that the student's answer is incorrect. However, we also can't confirm that it's correct because the context doesn't provide the necessary information.\\nGRADE: UNDETERMINABLE\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9bf6ba47-b26c-4873-b9be-7ce262bba110'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('949cdff0-cf86-41ed-8bb7-25763d58bced'), target_run_id=None)],\n",
              "   'execution_time': 1.958697,\n",
              "   'run_id': 'c138db02-28bf-40fd-bd00-da788456c13c',\n",
              "   'output': 'QLoRA (Quantized Low-Rank Adaptation) typically uses the AdamW optimizer. AdamW is a variant of the Adam optimizer that includes weight decay for regularization. This optimizer is well-suited for training large language models and is commonly used in fine-tuning tasks, including those involving quantized models like QLoRA.',\n",
              "   'reference': {'must_mention': ['paged', 'optimizer']}},\n",
              "  '05d84c9b-392e-400f-9bd7-6427fc8ebe2d': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a clear and direct answer to the question, stating that the QLoRA paper introduced a new data type called 4-bit NormalFloat (NF4). \\n\\nIn addition to providing the direct answer, the submission also provides additional information about the purpose and use of this new data type, stating that it is designed to be optimal for normally distributed weights and helps in reducing memory usage while preserving performance during the fine-tuning of large language models. \\n\\nThis additional information is relevant and useful for understanding the context and significance of the new data type, which makes the submission more helpful. \\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('23f8c647-8ba7-4c3d-ad68-374c6cc3040c'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The context mentions 'NF4' and 'NormalFloat' as the data type created in the QLoRA paper. The student's answer also mentions '4-bit NormalFloat (NF4)' as the data type introduced in the QLoRA paper. The student's answer aligns with the context provided. Therefore, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('63b21e80-1a82-4199-b60f-f0888dc85c58'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('5095d862-2126-4e7b-af3a-9b0b58846b21'), target_run_id=None)],\n",
              "   'execution_time': 3.497752,\n",
              "   'run_id': '5581b9a1-9e91-4683-af58-d8c0d76335e4',\n",
              "   'output': 'In the QLoRA paper, the authors introduced a new data type called **4-bit NormalFloat (NF4)**. This data type is designed to be information-theoretically optimal for normally distributed weights, which helps in reducing memory usage while preserving performance during the fine-tuning of large language models.',\n",
              "   'reference': {'must_mention': ['NF4', 'NormalFloat']}},\n",
              "  'dde2e3b8-08df-4632-98ae-5c22cd246c59': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". To assess this, we need to consider whether the submission provides useful, insightful, and appropriate information.\\n\\n1. **Usefulness**: The submission provides a detailed explanation of what a Retrieval Augmented Generation (RAG) system is, breaking down its components and how they work together. It also outlines the benefits of RAG systems and gives examples of their applications. This information is useful for someone looking to understand RAG systems.\\n\\n2. **Insightfulness**: The submission goes beyond a basic definition of a RAG system. It provides insights into the benefits of such systems and their potential applications, which adds depth to the explanation.\\n\\n3. **Appropriateness**: The submission is directly related to the input query and provides a comprehensive answer to it. The language used is professional and suitable for an explanation of a technical concept.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b90e75c0-e0a8-46e9-93d0-66f07bdd5fc9'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer provides a detailed explanation of what a Retrieval Augmented Generation (RAG) system is. The answer includes a breakdown of the components of a RAG system, the benefits of using such a system, and its applications. The student's answer aligns with the context provided and does not contain any conflicting statements. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('011e06e9-b21d-44f2-903d-9f8a1e01b5b3'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('5157bad2-8fd2-49d9-83ed-c49628fda765'), target_run_id=None)],\n",
              "   'execution_time': 8.496645,\n",
              "   'run_id': '7cdfb325-1b36-47ea-98c8-06c8fa7d6033',\n",
              "   'output': \"A Retrieval Augmented Generation (RAG) system is a type of artificial intelligence that combines retrieval-based and generation-based approaches to improve the quality and accuracy of generated responses. Here's a breakdown of its components and how it works:\\n\\n1. **Retrieval Component**: This part of the system searches a large corpus of documents or a database to find relevant information based on the input query. It retrieves the most pertinent pieces of information that can help answer the query.\\n\\n2. **Generation Component**: This part uses a generative model, such as a transformer-based language model (e.g., GPT-3), to generate a coherent and contextually appropriate response. The generative model can create new text based on the input and the retrieved information.\\n\\n3. **Integration**: The retrieved information is fed into the generative model to guide the generation process. This helps the model produce more accurate and informative responses by grounding its output in real, relevant data.\\n\\n### Benefits of RAG Systems\\n- **Improved Accuracy**: By leveraging external knowledge, RAG systems can provide more accurate and factually correct responses.\\n- **Contextual Relevance**: The retrieval component ensures that the generated responses are contextually relevant to the query.\\n- **Scalability**: These systems can handle a wide range of topics by accessing large and diverse datasets.\\n\\n### Applications\\n- **Question Answering**: Providing precise answers to user queries by retrieving relevant documents and generating a concise response.\\n- **Customer Support**: Assisting customer service agents by retrieving relevant information from knowledge bases and generating helpful responses.\\n- **Content Creation**: Aiding in the creation of articles, reports, or other content by retrieving relevant information and generating coherent text.\\n\\nOverall, RAG systems represent a powerful approach to combining the strengths of retrieval-based and generation-based methods to enhance the capabilities of AI in various applications.\",\n",
              "   'reference': {'must_mention': ['ground', 'context']}},\n",
              "  '87fac35a-6f02-439d-a52e-8d2fb3c1fcdd': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a clear and direct answer to the question asked, which is who authored the QLoRA paper. It lists all the authors of the paper, which is helpful for anyone seeking this information. \\n\\nAdditionally, the submission goes beyond the question asked and provides extra information about the title of the paper and the date it was published. This additional information could be helpful to someone who wants to look up the paper for further reading. \\n\\nTherefore, the submission is helpful, insightful, and appropriate, meeting the given criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b9e923f0-6b15-4285-b12d-797b83dfce2e'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The context only provides the name 'Tim Dettmers' as the author of the QLoRA paper. The student's answer includes 'Tim Dettmers' but also lists three additional authors. Since the context does not provide information about these additional authors, we cannot confirm if they are correct or not. However, the student's answer does not conflict with the context as it includes 'Tim Dettmers' as one of the authors. Therefore, based on the information provided, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8d265e9c-1a59-45ef-810f-6966eb4b3396'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('31ce5ba8-9fc5-4931-a716-2d9ef6a8031d'), target_run_id=None)],\n",
              "   'execution_time': 2.998058,\n",
              "   'run_id': 'f5b0bed0-4a5f-4848-b515-d56cfebed563',\n",
              "   'output': 'The QLoRA paper titled \"QLoRA: Efficient Finetuning of Quantized LLMs\" was authored by:\\n\\n- Tim Dettmers\\n- Artidoro Pagnoni\\n- Ari Holtzman\\n- Luke Zettlemoyer\\n\\nThe paper was published on May 23, 2023.',\n",
              "   'reference': {'must_mention': ['Tim', 'Dettmers']}},\n",
              "  '82cfbe1a-94f8-49ad-b396-ade720e4964b': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a comprehensive list of popular deep learning frameworks, including TensorFlow, PyTorch, Keras, Caffe, MXNet, and JAX. For each framework, the submission provides a brief description and context about its development and main features. This information is helpful for someone looking to understand the landscape of deep learning frameworks.\\n\\nMoreover, the submission acknowledges that the popularity of these frameworks can vary depending on different criteria, such as industry adoption, academic research, and community support. This insight is helpful as it provides a nuanced understanding of the question.\\n\\nFinally, the submission offers to perform a search for the most current and specific information, including trends and statistics. This offer is helpful as it shows a willingness to provide further assistance.\\n\\nBased on this analysis, the submission is helpful, insightful, and appropriate. Therefore, it meets the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b9c5a7ab-b2f0-42d1-8ebc-fb10975b5ba6'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is factually correct. The context mentions 'PyTorch' and 'TensorFlow' as popular deep learning frameworks. The student's answer includes both 'PyTorch' and 'TensorFlow' in their list of popular deep learning frameworks. The student also provides additional information about other popular deep learning frameworks, which does not conflict with the context. Therefore, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8b721e5c-8b71-41d6-b283-e34575db53be'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('6ecc5aa4-7365-4098-ad50-ad5e6b5ab1c1'), target_run_id=None)],\n",
              "   'execution_time': 5.567027,\n",
              "   'run_id': 'acbc8ba4-0692-470b-8817-e2c1cee766bc',\n",
              "   'output': \"The popularity of deep learning frameworks can vary depending on the criteria used (e.g., industry adoption, academic research, community support, etc.). However, as of the most recent data, the following frameworks are among the most popular:\\n\\n1. **TensorFlow**: Developed by Google, TensorFlow is widely used in both industry and academia. It has a comprehensive ecosystem and supports a wide range of applications.\\n\\n2. **PyTorch**: Developed by Facebook's AI Research lab, PyTorch has gained significant popularity, especially in the research community, due to its dynamic computation graph and ease of use.\\n\\n3. **Keras**: Initially an independent project, Keras is now part of the TensorFlow ecosystem. It is known for its simplicity and ease of use, making it a popular choice for beginners.\\n\\n4. **Caffe**: Developed by the Berkeley Vision and Learning Center (BVLC), Caffe is known for its speed and efficiency, particularly in image processing tasks.\\n\\n5. **MXNet**: An open-source deep learning framework developed by Apache, MXNet is known for its scalability and efficiency, particularly in distributed computing environments.\\n\\n6. **JAX**: Developed by Google, JAX is gaining traction for its ability to transform numerical functions into high-performance, GPU/TPU-executable code.\\n\\nTo get the most current and specific information, including trends and statistics, I can perform a search. Would you like me to do that?\",\n",
              "   'reference': {'must_mention': ['PyTorch', 'TensorFlow']}},\n",
              "  '35f520f0-6e98-4915-8317-854b9a41fec7': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, it provides a detailed explanation of the improvements that the LoRA system brings. It lists seven significant improvements, explaining each one in a clear and understandable manner. This makes the submission helpful as it provides the information needed to understand the question.\\n\\nThe submission is also insightful. It not only lists the improvements but also explains why these improvements are significant. For example, it explains how the parameter efficiency of LoRA makes the fine-tuning process more efficient, and how its modularity is useful for multi-task learning and transfer learning scenarios.\\n\\nLastly, the submission is appropriate. It directly answers the question and stays on topic throughout. It uses professional language and provides a comprehensive answer to the question.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('26817b0d-fc18-45cc-b782-e144a185ab2a'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The context provided is 'reduce' and 'parameters'. The student's answer mentions that LoRA significantly reduces the number of trainable parameters required for fine-tuning, which aligns with the context provided. The student's answer also provides additional information about the benefits of LoRA, but none of this information contradicts the context or the question. Therefore, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ebca7db5-dc64-4dbd-8fd6-0e0d2129e095'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('319bb764-4292-424f-b021-f7caa700d0e8'), target_run_id=None)],\n",
              "   'execution_time': 9.235983,\n",
              "   'run_id': 'f4758ef2-ca47-4406-aa6b-0552dcafcea1',\n",
              "   'output': 'LoRA, or Low-Rank Adaptation, is a technique used to fine-tune large language models (LLMs) efficiently. Here are some significant improvements that LoRA brings to the table:\\n\\n1. **Parameter Efficiency**: LoRA significantly reduces the number of trainable parameters required for fine-tuning. Instead of updating all the parameters of the model, LoRA introduces a small number of additional parameters that capture the necessary adaptations. This makes the fine-tuning process much more parameter-efficient.\\n\\n2. **Memory Efficiency**: By reducing the number of parameters that need to be updated, LoRA also reduces the memory footprint during training. This allows for fine-tuning on hardware with limited memory resources, such as GPUs with smaller VRAM.\\n\\n3. **Training Speed**: With fewer parameters to update, the training process becomes faster. This is particularly beneficial when working with very large models, where full fine-tuning can be computationally expensive and time-consuming.\\n\\n4. **Modularity**: LoRA allows for modular updates to the model. This means that different tasks can have their own low-rank adaptations, which can be combined or swapped out as needed. This modularity is useful for multi-task learning and transfer learning scenarios.\\n\\n5. **Preservation of Pre-trained Knowledge**: Since LoRA updates only a small subset of parameters, the original pre-trained knowledge of the model is largely preserved. This helps in maintaining the generalization capabilities of the model while adapting it to specific tasks.\\n\\n6. **Scalability**: LoRA can be applied to very large models, making it scalable to state-of-the-art LLMs with billions of parameters. This scalability is crucial for leveraging the power of large models without incurring prohibitive computational costs.\\n\\n7. **Flexibility**: LoRA can be used in conjunction with other fine-tuning techniques, providing flexibility in how models are adapted to new tasks. It can be integrated into existing training pipelines with minimal changes.\\n\\nOverall, LoRA provides a more efficient and scalable way to fine-tune large language models, making it a valuable tool for researchers and practitioners working with LLMs.',\n",
              "   'reference': {'must_mention': ['reduce', 'parameters']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ],
      "metadata": {
        "id": "jhTNe4kWrplB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add a custom node and conditional edge to determine if the response was helpful enough."
      ],
      "metadata": {
        "id": "w1wKRddbIY_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ],
      "metadata": {
        "id": "npTYJ8ayR5B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ],
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to add a custom helpfulness check here!"
      ],
      "metadata": {
        "id": "gC8t-4FISCEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def check_helpfulness(state):\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    print(\"Helpful!\")\n",
        "    return \"end\"\n",
        "  else:\n",
        "    print(\"Not helpful!\")\n",
        "    return \"continue\"\n",
        "\n",
        "def dummy_node(state):\n",
        "  return"
      ],
      "metadata": {
        "id": "ZV_PxI5zNY7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ],
      "metadata": {
        "id": "sD7EV0HqSQcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####🏗️ Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ],
      "metadata": {
        "id": "oajBwLkFVi1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### YOUR MARKDOWN HERE"
      ],
      "metadata": {
        "id": "M6rN7feNVn9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", call_tool)\n",
        "graph_with_helpfulness_check.add_node(\"passthrough\", dummy_node)"
      ],
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### YOUR MARKDOWN HERE"
      ],
      "metadata": {
        "id": "XZ22o2mWVrfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ],
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### YOUR MARKDOWN HERE"
      ],
      "metadata": {
        "id": "6BhnBW2YVsJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : \"passthrough\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"passthrough\",\n",
        "    check_helpfulness,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### YOUR MARKDOWN HERE"
      ],
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ],
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### YOUR MARKDOWN HERE"
      ],
      "metadata": {
        "id": "rSI8AOaEVvT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ],
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### YOUR MARKDOWN HERE"
      ],
      "metadata": {
        "id": "F67FGCMRVwGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What state did Elon Musk make a complaint against OpenAI? And did he really claim that OpenAI had achieved AGI?\")]}\n",
        "\n",
        "messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "710b20eb-0fd9-49d7-d96b-86cf7b11e865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='' additional_kwargs={'function_call': {'arguments': '{\"query\":\"Elon Musk complaint against OpenAI state and AGI claim\"}', 'name': 'retrieve_information'}} response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 207, 'total_tokens': 232}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_298125635f', 'finish_reason': 'function_call', 'logprobs': None} id='run-70b28434-3d03-41fd-9b8d-4852f4e5ae2d-0' usage_metadata={'input_tokens': 207, 'output_tokens': 25, 'total_tokens': 232}\n",
            "content='' additional_kwargs={'function_call': {'arguments': '{\"query\":\"Elon Musk complaint against OpenAI state and AGI claim\"}', 'name': 'retrieve_information'}} response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 207, 'total_tokens': 232}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_298125635f', 'finish_reason': 'function_call', 'logprobs': None} id='run-70b28434-3d03-41fd-9b8d-4852f4e5ae2d-0' usage_metadata={'input_tokens': 207, 'output_tokens': 25, 'total_tokens': 232}\n",
            "content=\"Elon Musk's complaint against OpenAI involves concerns over the development of Artificial General Intelligence (AGI). He claims that OpenAI is developing a model known as Q* (Q star), which he believes has a strong claim to AGI. The complaint aims to compel OpenAI to adhere to its original mission of developing AGI for the benefit of humanity, rather than for personal or corporate gain.\" response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 337, 'total_tokens': 418}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_dd932ca5d1', 'finish_reason': 'stop', 'logprobs': None} id='run-d1a408d2-86be-4521-8a0f-e1188b0ccb3c-0' usage_metadata={'input_tokens': 337, 'output_tokens': 81, 'total_tokens': 418}\n",
            "content=\"Elon Musk's complaint against OpenAI involves concerns over the development of Artificial General Intelligence (AGI). He claims that OpenAI is developing a model known as Q* (Q star), which he believes has a strong claim to AGI. The complaint aims to compel OpenAI to adhere to its original mission of developing AGI for the benefit of humanity, rather than for personal or corporate gain.\" response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 337, 'total_tokens': 418}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_dd932ca5d1', 'finish_reason': 'stop', 'logprobs': None} id='run-d1a408d2-86be-4521-8a0f-e1188b0ccb3c-0' usage_metadata={'input_tokens': 337, 'output_tokens': 81, 'total_tokens': 418}\n",
            "Helpful!\n",
            "Initial Query: What state did Elon Musk make a complaint against OpenAI? And did he really claim that OpenAI had achieved AGI?\n",
            "\n",
            "\n",
            "Tool Call - Name: retrieve_information + Query: {\"query\":\"Elon Musk complaint against OpenAI state and AGI claim\"}\n",
            "Tool Response: Elon Musk's complaint against OpenAI states that he shared concerns over the threat posed by Artificial General Intelligence (AGI), and that OpenAI is developing a model known as Q* (Q star) that has an even stronger claim to AGI. This case is filed to compel OpenAI to adhere to the Founding Agreement and return to its mission to develop AGI for the benefit of humanity, not to personally benefit the individual defendants and the largest technology company in the world.\n",
            "\n",
            "Agent Response: Elon Musk's complaint against OpenAI involves concerns over the development of Artificial General Intelligence (AGI). He claims that OpenAI is developing a model known as Q* (Q star), which he believes has a strong claim to AGI. The complaint aims to compel OpenAI to adhere to its original mission of developing AGI for the benefit of humanity, rather than for personal or corporate gain.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ],
      "metadata": {
        "id": "yVmZPs6lnpsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ],
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print_messages(messages)\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "5223fefb-7e50-4793-c8b1-f9269afe21a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"Prompt engineering is a technique used primarily in the field of artificial intelligence (AI) and natural language processing (NLP) to design and refine prompts that elicit desired responses from language models. The goal is to craft inputs (prompts) that guide the model to generate useful, relevant, and accurate outputs. This involves understanding the model's behavior and iteratively adjusting the prompts to improve performance on specific tasks.\\n\\nPrompt engineering became more prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. These models demonstrated significant capabilities in generating human-like text, answering questions, and performing various language tasks, but their performance heavily depended on how they were prompted. As a result, the practice of prompt engineering emerged as a crucial skill for leveraging the full potential of these models.\\n\\nThe concept of prompt engineering, however, has roots in earlier AI and NLP research, where designing effective inputs for rule-based systems and simpler machine learning models was also important. But it was the capabilities and flexibility of modern large language models that brought prompt engineering into the spotlight as a distinct and valuable practice.\" response_metadata={'token_usage': {'completion_tokens': 227, 'prompt_tokens': 195, 'total_tokens': 422}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None} id='run-8a5947fc-9b26-4821-bc3d-95434b64622a-0' usage_metadata={'input_tokens': 195, 'output_tokens': 227, 'total_tokens': 422}\n",
            "content=\"Prompt engineering is a technique used primarily in the field of artificial intelligence (AI) and natural language processing (NLP) to design and refine prompts that elicit desired responses from language models. The goal is to craft inputs (prompts) that guide the model to generate useful, relevant, and accurate outputs. This involves understanding the model's behavior and iteratively adjusting the prompts to improve performance on specific tasks.\\n\\nPrompt engineering became more prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. These models demonstrated significant capabilities in generating human-like text, answering questions, and performing various language tasks, but their performance heavily depended on how they were prompted. As a result, the practice of prompt engineering emerged as a crucial skill for leveraging the full potential of these models.\\n\\nThe concept of prompt engineering, however, has roots in earlier AI and NLP research, where designing effective inputs for rule-based systems and simpler machine learning models was also important. But it was the capabilities and flexibility of modern large language models that brought prompt engineering into the spotlight as a distinct and valuable practice.\" response_metadata={'token_usage': {'completion_tokens': 227, 'prompt_tokens': 195, 'total_tokens': 422}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None} id='run-8a5947fc-9b26-4821-bc3d-95434b64622a-0' usage_metadata={'input_tokens': 195, 'output_tokens': 227, 'total_tokens': 422}\n",
            "Helpful!\n",
            "Initial Query: What is prompt engineering and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: Prompt engineering is a technique used primarily in the field of artificial intelligence (AI) and natural language processing (NLP) to design and refine prompts that elicit desired responses from language models. The goal is to craft inputs (prompts) that guide the model to generate useful, relevant, and accurate outputs. This involves understanding the model's behavior and iteratively adjusting the prompts to improve performance on specific tasks.\n",
            "\n",
            "Prompt engineering became more prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. These models demonstrated significant capabilities in generating human-like text, answering questions, and performing various language tasks, but their performance heavily depended on how they were prompted. As a result, the practice of prompt engineering emerged as a crucial skill for leveraging the full potential of these models.\n",
            "\n",
            "The concept of prompt engineering, however, has roots in earlier AI and NLP research, where designing effective inputs for rule-based systems and simpler machine learning models was also important. But it was the capabilities and flexibility of modern large language models that brought prompt engineering into the spotlight as a distinct and valuable practice.\n",
            "\n",
            "\n",
            "\n",
            "content='RAG stands for Retrieval-Augmented Generation. It is a technique in natural language processing (NLP) that combines the strengths of retrieval-based and generation-based models to improve the quality and relevance of generated text.\\n\\n### Key Components of RAG:\\n1. **Retrieval Component**: This part of the model retrieves relevant documents or passages from a large corpus based on the input query.\\n2. **Generation Component**: This part generates a response or continuation based on the retrieved documents and the input query.\\n\\n### How RAG Works:\\n1. **Input Query**: The user provides an input query.\\n2. **Document Retrieval**: The retrieval component searches a large corpus to find documents or passages that are relevant to the query.\\n3. **Response Generation**: The generation component uses the retrieved documents to generate a coherent and contextually relevant response.\\n\\n### Advantages of RAG:\\n- **Improved Relevance**: By leveraging external documents, RAG can provide more accurate and contextually appropriate responses.\\n- **Knowledge Integration**: It can integrate up-to-date information from a large corpus, making it useful for tasks that require current knowledge.\\n- **Flexibility**: It can be applied to various NLP tasks, including question answering, summarization, and dialogue systems.\\n\\n### When Did RAG Break Onto the Scene?\\nRAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The paper demonstrated the effectiveness of RAG in various knowledge-intensive tasks and highlighted its potential to improve the performance of NLP models by combining retrieval and generation techniques.' response_metadata={'token_usage': {'completion_tokens': 337, 'prompt_tokens': 195, 'total_tokens': 532}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_dd932ca5d1', 'finish_reason': 'stop', 'logprobs': None} id='run-530a85fc-8441-4cb3-bcaa-96144ee3c9ab-0' usage_metadata={'input_tokens': 195, 'output_tokens': 337, 'total_tokens': 532}\n",
            "content='RAG stands for Retrieval-Augmented Generation. It is a technique in natural language processing (NLP) that combines the strengths of retrieval-based and generation-based models to improve the quality and relevance of generated text.\\n\\n### Key Components of RAG:\\n1. **Retrieval Component**: This part of the model retrieves relevant documents or passages from a large corpus based on the input query.\\n2. **Generation Component**: This part generates a response or continuation based on the retrieved documents and the input query.\\n\\n### How RAG Works:\\n1. **Input Query**: The user provides an input query.\\n2. **Document Retrieval**: The retrieval component searches a large corpus to find documents or passages that are relevant to the query.\\n3. **Response Generation**: The generation component uses the retrieved documents to generate a coherent and contextually relevant response.\\n\\n### Advantages of RAG:\\n- **Improved Relevance**: By leveraging external documents, RAG can provide more accurate and contextually appropriate responses.\\n- **Knowledge Integration**: It can integrate up-to-date information from a large corpus, making it useful for tasks that require current knowledge.\\n- **Flexibility**: It can be applied to various NLP tasks, including question answering, summarization, and dialogue systems.\\n\\n### When Did RAG Break Onto the Scene?\\nRAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The paper demonstrated the effectiveness of RAG in various knowledge-intensive tasks and highlighted its potential to improve the performance of NLP models by combining retrieval and generation techniques.' response_metadata={'token_usage': {'completion_tokens': 337, 'prompt_tokens': 195, 'total_tokens': 532}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_dd932ca5d1', 'finish_reason': 'stop', 'logprobs': None} id='run-530a85fc-8441-4cb3-bcaa-96144ee3c9ab-0' usage_metadata={'input_tokens': 195, 'output_tokens': 337, 'total_tokens': 532}\n",
            "Helpful!\n",
            "Initial Query: What is RAG and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: RAG stands for Retrieval-Augmented Generation. It is a technique in natural language processing (NLP) that combines the strengths of retrieval-based and generation-based models to improve the quality and relevance of generated text.\n",
            "\n",
            "### Key Components of RAG:\n",
            "1. **Retrieval Component**: This part of the model retrieves relevant documents or passages from a large corpus based on the input query.\n",
            "2. **Generation Component**: This part generates a response or continuation based on the retrieved documents and the input query.\n",
            "\n",
            "### How RAG Works:\n",
            "1. **Input Query**: The user provides an input query.\n",
            "2. **Document Retrieval**: The retrieval component searches a large corpus to find documents or passages that are relevant to the query.\n",
            "3. **Response Generation**: The generation component uses the retrieved documents to generate a coherent and contextually relevant response.\n",
            "\n",
            "### Advantages of RAG:\n",
            "- **Improved Relevance**: By leveraging external documents, RAG can provide more accurate and contextually appropriate responses.\n",
            "- **Knowledge Integration**: It can integrate up-to-date information from a large corpus, making it useful for tasks that require current knowledge.\n",
            "- **Flexibility**: It can be applied to various NLP tasks, including question answering, summarization, and dialogue systems.\n",
            "\n",
            "### When Did RAG Break Onto the Scene?\n",
            "RAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The paper demonstrated the effectiveness of RAG in various knowledge-intensive tasks and highlighted its potential to improve the performance of NLP models by combining retrieval and generation techniques.\n",
            "\n",
            "\n",
            "\n",
            "content=\"Fine-tuning is a process in machine learning where a pre-trained model is further trained on a specific task or dataset to improve its performance on that task. This approach leverages the general knowledge the model has already acquired during its initial training on a large and diverse dataset, and then refines it to better suit the specific requirements of the new task.\\n\\n### Key Steps in Fine-Tuning:\\n1. **Pre-training**: A model is initially trained on a large dataset, often using unsupervised learning techniques. This helps the model learn general features and patterns.\\n2. **Fine-tuning**: The pre-trained model is then trained on a smaller, task-specific dataset. This step adjusts the model's parameters to better fit the new data and task.\\n\\n### Benefits of Fine-Tuning:\\n- **Efficiency**: It requires less data and computational resources compared to training a model from scratch.\\n- **Performance**: Fine-tuned models often achieve better performance on specific tasks because they start with a strong foundation of general knowledge.\\n- **Adaptability**: It allows models to be adapted to a wide range of tasks with relatively minor adjustments.\\n\\n### Historical Context:\\nFine-tuning became particularly prominent with the advent of deep learning and the development of large-scale neural networks. Some key milestones include:\\n\\n- **Transfer Learning**: The concept of transfer learning, which underpins fine-tuning, has been around for decades. However, it gained significant attention with the success of deep learning models.\\n- **ImageNet**: The success of models like AlexNet (2012) on the ImageNet dataset demonstrated the power of pre-training on large datasets and fine-tuning for specific tasks.\\n- **NLP Models**: The introduction of models like BERT (2018) and GPT (2018) in natural language processing (NLP) showcased the effectiveness of fine-tuning pre-trained language models for various NLP tasks.\\n\\nFine-tuning has since become a standard practice in machine learning, particularly in fields like computer vision and natural language processing, where large pre-trained models are commonly used as starting points for specialized applications.\" response_metadata={'token_usage': {'completion_tokens': 425, 'prompt_tokens': 196, 'total_tokens': 621}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None} id='run-937aa5f6-71b3-4ed4-943f-4e25935fe674-0' usage_metadata={'input_tokens': 196, 'output_tokens': 425, 'total_tokens': 621}\n",
            "content=\"Fine-tuning is a process in machine learning where a pre-trained model is further trained on a specific task or dataset to improve its performance on that task. This approach leverages the general knowledge the model has already acquired during its initial training on a large and diverse dataset, and then refines it to better suit the specific requirements of the new task.\\n\\n### Key Steps in Fine-Tuning:\\n1. **Pre-training**: A model is initially trained on a large dataset, often using unsupervised learning techniques. This helps the model learn general features and patterns.\\n2. **Fine-tuning**: The pre-trained model is then trained on a smaller, task-specific dataset. This step adjusts the model's parameters to better fit the new data and task.\\n\\n### Benefits of Fine-Tuning:\\n- **Efficiency**: It requires less data and computational resources compared to training a model from scratch.\\n- **Performance**: Fine-tuned models often achieve better performance on specific tasks because they start with a strong foundation of general knowledge.\\n- **Adaptability**: It allows models to be adapted to a wide range of tasks with relatively minor adjustments.\\n\\n### Historical Context:\\nFine-tuning became particularly prominent with the advent of deep learning and the development of large-scale neural networks. Some key milestones include:\\n\\n- **Transfer Learning**: The concept of transfer learning, which underpins fine-tuning, has been around for decades. However, it gained significant attention with the success of deep learning models.\\n- **ImageNet**: The success of models like AlexNet (2012) on the ImageNet dataset demonstrated the power of pre-training on large datasets and fine-tuning for specific tasks.\\n- **NLP Models**: The introduction of models like BERT (2018) and GPT (2018) in natural language processing (NLP) showcased the effectiveness of fine-tuning pre-trained language models for various NLP tasks.\\n\\nFine-tuning has since become a standard practice in machine learning, particularly in fields like computer vision and natural language processing, where large pre-trained models are commonly used as starting points for specialized applications.\" response_metadata={'token_usage': {'completion_tokens': 425, 'prompt_tokens': 196, 'total_tokens': 621}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None} id='run-937aa5f6-71b3-4ed4-943f-4e25935fe674-0' usage_metadata={'input_tokens': 196, 'output_tokens': 425, 'total_tokens': 621}\n",
            "Helpful!\n",
            "Initial Query: What is fine-tuning and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: Fine-tuning is a process in machine learning where a pre-trained model is further trained on a specific task or dataset to improve its performance on that task. This approach leverages the general knowledge the model has already acquired during its initial training on a large and diverse dataset, and then refines it to better suit the specific requirements of the new task.\n",
            "\n",
            "### Key Steps in Fine-Tuning:\n",
            "1. **Pre-training**: A model is initially trained on a large dataset, often using unsupervised learning techniques. This helps the model learn general features and patterns.\n",
            "2. **Fine-tuning**: The pre-trained model is then trained on a smaller, task-specific dataset. This step adjusts the model's parameters to better fit the new data and task.\n",
            "\n",
            "### Benefits of Fine-Tuning:\n",
            "- **Efficiency**: It requires less data and computational resources compared to training a model from scratch.\n",
            "- **Performance**: Fine-tuned models often achieve better performance on specific tasks because they start with a strong foundation of general knowledge.\n",
            "- **Adaptability**: It allows models to be adapted to a wide range of tasks with relatively minor adjustments.\n",
            "\n",
            "### Historical Context:\n",
            "Fine-tuning became particularly prominent with the advent of deep learning and the development of large-scale neural networks. Some key milestones include:\n",
            "\n",
            "- **Transfer Learning**: The concept of transfer learning, which underpins fine-tuning, has been around for decades. However, it gained significant attention with the success of deep learning models.\n",
            "- **ImageNet**: The success of models like AlexNet (2012) on the ImageNet dataset demonstrated the power of pre-training on large datasets and fine-tuning for specific tasks.\n",
            "- **NLP Models**: The introduction of models like BERT (2018) and GPT (2018) in natural language processing (NLP) showcased the effectiveness of fine-tuning pre-trained language models for various NLP tasks.\n",
            "\n",
            "Fine-tuning has since become a standard practice in machine learning, particularly in fields like computer vision and natural language processing, where large pre-trained models are commonly used as starting points for specialized applications.\n",
            "\n",
            "\n",
            "\n",
            "content=\"LLM-based agents, or Large Language Model-based agents, are artificial intelligence systems that leverage large language models to perform a variety of tasks. These models are trained on vast amounts of text data and can understand, generate, and manipulate human language with a high degree of proficiency. They are used in applications such as chatbots, virtual assistants, content generation, translation, summarization, and more.\\n\\n### Key Characteristics of LLM-based Agents:\\n1. **Natural Language Understanding (NLU):** They can comprehend and interpret human language in a way that is contextually relevant.\\n2. **Natural Language Generation (NLG):** They can generate human-like text based on the input they receive.\\n3. **Versatility:** They can be fine-tuned for specific tasks or domains, making them highly adaptable.\\n4. **Scalability:** They can handle large-scale data and perform complex computations efficiently.\\n\\n### Historical Context:\\n- **Early Developments:** The concept of language models has been around for decades, but the significant breakthroughs came with the advent of deep learning and neural networks.\\n- **Transformers:** The introduction of the Transformer architecture by Vaswani et al. in 2017 was a pivotal moment. This architecture allowed for more efficient training and better performance on language tasks.\\n- **GPT Series:** OpenAI's Generative Pre-trained Transformer (GPT) models, starting with GPT-1 in 2018, followed by GPT-2 in 2019, and GPT-3 in 2020, marked significant milestones. These models demonstrated unprecedented capabilities in generating coherent and contextually relevant text.\\n- **Widespread Adoption:** By the early 2020s, LLM-based agents began to be widely adopted in various industries, from customer service to content creation, due to their ability to perform a wide range of language-related tasks.\\n\\n### Breakthrough Moments:\\n- **GPT-3 Release (2020):** The release of GPT-3 by OpenAI was a major breakthrough. With 175 billion parameters, it showcased the potential of LLMs in generating human-like text and performing complex language tasks.\\n- **ChatGPT (2020):** OpenAI's ChatGPT, based on GPT-3, became widely popular for its conversational abilities, bringing LLM-based agents into mainstream awareness.\\n- **BERT and Other Models:** Google's BERT (Bidirectional Encoder Representations from Transformers) and other models like T5 and RoBERTa also contributed to the advancements in LLMs.\\n\\nIn summary, LLM-based agents broke onto the scene primarily in the late 2010s and early 2020s, driven by advancements in neural network architectures, particularly the Transformer model, and the development of highly capable models like GPT-3.\" response_metadata={'token_usage': {'completion_tokens': 557, 'prompt_tokens': 197, 'total_tokens': 754}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None} id='run-b90187ff-e6f1-4787-b8f2-c695c2d0965a-0' usage_metadata={'input_tokens': 197, 'output_tokens': 557, 'total_tokens': 754}\n",
            "content=\"LLM-based agents, or Large Language Model-based agents, are artificial intelligence systems that leverage large language models to perform a variety of tasks. These models are trained on vast amounts of text data and can understand, generate, and manipulate human language with a high degree of proficiency. They are used in applications such as chatbots, virtual assistants, content generation, translation, summarization, and more.\\n\\n### Key Characteristics of LLM-based Agents:\\n1. **Natural Language Understanding (NLU):** They can comprehend and interpret human language in a way that is contextually relevant.\\n2. **Natural Language Generation (NLG):** They can generate human-like text based on the input they receive.\\n3. **Versatility:** They can be fine-tuned for specific tasks or domains, making them highly adaptable.\\n4. **Scalability:** They can handle large-scale data and perform complex computations efficiently.\\n\\n### Historical Context:\\n- **Early Developments:** The concept of language models has been around for decades, but the significant breakthroughs came with the advent of deep learning and neural networks.\\n- **Transformers:** The introduction of the Transformer architecture by Vaswani et al. in 2017 was a pivotal moment. This architecture allowed for more efficient training and better performance on language tasks.\\n- **GPT Series:** OpenAI's Generative Pre-trained Transformer (GPT) models, starting with GPT-1 in 2018, followed by GPT-2 in 2019, and GPT-3 in 2020, marked significant milestones. These models demonstrated unprecedented capabilities in generating coherent and contextually relevant text.\\n- **Widespread Adoption:** By the early 2020s, LLM-based agents began to be widely adopted in various industries, from customer service to content creation, due to their ability to perform a wide range of language-related tasks.\\n\\n### Breakthrough Moments:\\n- **GPT-3 Release (2020):** The release of GPT-3 by OpenAI was a major breakthrough. With 175 billion parameters, it showcased the potential of LLMs in generating human-like text and performing complex language tasks.\\n- **ChatGPT (2020):** OpenAI's ChatGPT, based on GPT-3, became widely popular for its conversational abilities, bringing LLM-based agents into mainstream awareness.\\n- **BERT and Other Models:** Google's BERT (Bidirectional Encoder Representations from Transformers) and other models like T5 and RoBERTa also contributed to the advancements in LLMs.\\n\\nIn summary, LLM-based agents broke onto the scene primarily in the late 2010s and early 2020s, driven by advancements in neural network architectures, particularly the Transformer model, and the development of highly capable models like GPT-3.\" response_metadata={'token_usage': {'completion_tokens': 557, 'prompt_tokens': 197, 'total_tokens': 754}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'stop', 'logprobs': None} id='run-b90187ff-e6f1-4787-b8f2-c695c2d0965a-0' usage_metadata={'input_tokens': 197, 'output_tokens': 557, 'total_tokens': 754}\n",
            "Helpful!\n",
            "Initial Query: What is LLM-based agents and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: LLM-based agents, or Large Language Model-based agents, are artificial intelligence systems that leverage large language models to perform a variety of tasks. These models are trained on vast amounts of text data and can understand, generate, and manipulate human language with a high degree of proficiency. They are used in applications such as chatbots, virtual assistants, content generation, translation, summarization, and more.\n",
            "\n",
            "### Key Characteristics of LLM-based Agents:\n",
            "1. **Natural Language Understanding (NLU):** They can comprehend and interpret human language in a way that is contextually relevant.\n",
            "2. **Natural Language Generation (NLG):** They can generate human-like text based on the input they receive.\n",
            "3. **Versatility:** They can be fine-tuned for specific tasks or domains, making them highly adaptable.\n",
            "4. **Scalability:** They can handle large-scale data and perform complex computations efficiently.\n",
            "\n",
            "### Historical Context:\n",
            "- **Early Developments:** The concept of language models has been around for decades, but the significant breakthroughs came with the advent of deep learning and neural networks.\n",
            "- **Transformers:** The introduction of the Transformer architecture by Vaswani et al. in 2017 was a pivotal moment. This architecture allowed for more efficient training and better performance on language tasks.\n",
            "- **GPT Series:** OpenAI's Generative Pre-trained Transformer (GPT) models, starting with GPT-1 in 2018, followed by GPT-2 in 2019, and GPT-3 in 2020, marked significant milestones. These models demonstrated unprecedented capabilities in generating coherent and contextually relevant text.\n",
            "- **Widespread Adoption:** By the early 2020s, LLM-based agents began to be widely adopted in various industries, from customer service to content creation, due to their ability to perform a wide range of language-related tasks.\n",
            "\n",
            "### Breakthrough Moments:\n",
            "- **GPT-3 Release (2020):** The release of GPT-3 by OpenAI was a major breakthrough. With 175 billion parameters, it showcased the potential of LLMs in generating human-like text and performing complex language tasks.\n",
            "- **ChatGPT (2020):** OpenAI's ChatGPT, based on GPT-3, became widely popular for its conversational abilities, bringing LLM-based agents into mainstream awareness.\n",
            "- **BERT and Other Models:** Google's BERT (Bidirectional Encoder Representations from Transformers) and other models like T5 and RoBERTa also contributed to the advancements in LLMs.\n",
            "\n",
            "In summary, LLM-based agents broke onto the scene primarily in the late 2010s and early 2020s, driven by advancements in neural network architectures, particularly the Transformer model, and the development of highly capable models like GPT-3.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}